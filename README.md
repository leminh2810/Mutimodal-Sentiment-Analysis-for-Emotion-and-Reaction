# ğŸ­ Multimodal Sentiment Analysis for Emotion and Reaction

This repository contains the implementation of a **multimodal emotion recognition system** combining **audio** and **visual** information for classifying human emotions.  
It integrates CNN-based feature extraction (EfficientFace, Conv1D) with **cross-modal fusion mechanisms** (Intermediate Attention and Late Transformer) for improved emotional understanding.

---

## ğŸ§  Overview

The system aims to recognize human emotions from short video clips by combining two modalities:
- **Visual stream**: facial expressions extracted from video frames.
- **Audio stream**: Mel-frequency cepstral coefficients (MFCC) features extracted from speech.

Two fusion strategies were implemented and compared:
1. **Intermediate Attention (IA)** â€” introduces cross-attention and temporal gating between modalities.
2. **Late Transformer (LT)** â€” merges embeddings via Transformer-based fusion after independent processing.

---

## âš™ï¸ Architecture

**1. Input Processing**
- Video â†’ detected faces (MTCNN) â†’ EfficientFace â†’ Conv1D temporal modeling.  
- Audio â†’ MFCC extraction â†’ Conv1D temporal modeling.

**2. Fusion**
- IA: bidirectional cross-attention + temporal gate  
- LT: late-stage Transformer encoder

**3. Output**
- Fully Connected â†’ Softmax layer â†’ Emotion classification (8 classes)

---

## ğŸ“Š Dataset

- **RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)**
- 24 professional actors, 8 emotion categories:
  - Neutral, Calm, Happy, Sad, Angry, Fearful, Disgust, Surprise
- 15-frame video clips synchronized with 1â€“2 second audio segments.

---

## ğŸš€ Training & Evaluation

| Configuration | Accuracy | Macro F1 |
|----------------|-----------|-----------|
| Audio-only | 45.2% | 43.8% |
| Video-only | 44.7% | 43.1% |
| IA Fusion | **79.8%** | **78.5%** |
| LT Fusion | 77.6% | 76.3% |

- Loss: CrossEntropy  
- Optimizer: Adam (lr=1e-4)  
- Batch size: 8, Epochs: 50  
- Hardware: NVIDIA GPU (â‰¥12GB VRAM recommended)

---

## ğŸ“ Project Structure

```
akaaka_mutil_model_fullcode/
â”‚
â”œâ”€â”€ main.py                # Main training script
â”œâ”€â”€ validation.py          # Model evaluation
â”œâ”€â”€ dataset.py             # RAVDESS data loading and preprocessing
â”œâ”€â”€ model.py               # CNN + Transformer architecture
â”œâ”€â”€ utils.py               # Helper functions
â”œâ”€â”€ results/               # Logs, metrics, confusion matrices
â”œâ”€â”€ ravdess_preprocessing/ # Audio/video extraction scripts
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ README.md              # This file
```

---

## ğŸ§© Requirements

Install dependencies:
```bash
pip install -r requirements.txt
```

---

## ğŸ’¡ Usage

### 1. Training
```bash
python main.py
```

### 2. Validation
```bash
python validation.py
```

### 3. Run with specific fusion mode
```bash
python main_aka.py --fusion IA     # Intermediate Attention
python main_aka.py --fusion LT     # Late Transformer
```

---

## ğŸ§  Citation

If you use or reference this work, please cite:
```
@project{leminh2025multimodal,
  title={Multimodal Sentiment Analysis for Emotion and Reaction},
  author={Le Minh, Le Chau Giang},
  year={2025},
  note={RAVDESS-based multimodal fusion for emotion recognition}
}
```

---

## ğŸª´ Acknowledgements

- [RAVDESS Dataset](https://zenodo.org/record/1188976)
- [EfficientFace model](https://github.com/zengqunzhao/EfficientFace)
- [PyTorch Image Models (timm)](https://github.com/rwightman/pytorch-image-models)

---

## ğŸ“« Contact

For academic or research collaboration:
**Le Minh** â€“ [GitHub Profile](https://github.com/leminh2810)
